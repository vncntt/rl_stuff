11/25:
- Went through first two stable RL tutorials 
   - Getting started
   - Code with creating new environments, importing policies, and initializing model with policy, env
   - Training and evaluating model
   - Recording video of trained model
- Gym wrappers
   - wrap environments on different constraints such as limiting number of episodes, normalizing actions, 
   - important wrappers: Monitor, DummyVecEnv
   - coded and tested custom wrapper

11/26:
- Multiprocessing
   - still donâ€™t really get
   - something to do with optimization between # of training envs and # of steps

11/27:
   - Callbacks
      - custom monitoring and saving models
      - important for saving models with best reward, keeping track of performance

11/28:
   - Worked through custom gyms tutorial
   - Implemented custom tictactoe environment

11/29:
   - cleaned up some code of tictactoe environment
   - went through tutorial of q-learning blackjack agent

11/30:
   - used baselines for a small ant env, recorded results
   - read math + pseudocode of ppo


12/1:
   - went through code of actual ppo 
      - there are so many specific weird details
      - don't fully understand the code

12/2:
   - went through ppo atari code
   - i keep getting env name doesn't exist. version error?

12/3:
   - no work bc of veritasium work + finals :(

12/4: no work bc i was being a bum 

12/5: 
- read and wrote down some notes for survey on demonstrations



wait given that i know how to write environments in the RL gym, i should be able to create 
an env with the apple game and train an agent to solve it right?