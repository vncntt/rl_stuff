{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install all system dependencies (with -y to auto-confirm)\n",
    "!apt-get update\n",
    "!apt-get install -y xvfb x11-xserver-utils\n",
    "!apt-get install -y \\\n",
    "    libx11-6 \\\n",
    "    libxau6 \\\n",
    "    libxdmcp6 \\\n",
    "    libxcb1 \\\n",
    "    libxext6 \\\n",
    "    libx11-xcb1 \\\n",
    "    libvulkan1 \\\n",
    "    vulkan-utils \\\n",
    "    libvulkan-dev \\\n",
    "    mesa-vulkan-drivers\n",
    "\n",
    "# 2. Install Python packages\n",
    "!pip install --upgrade mani_skill tyro pyvirtualdisplay\n",
    "\n",
    "# 3. Verify Xvfb is installed\n",
    "!which Xvfb\n",
    "\n",
    "# 4. Setup virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "virtual_display = Display(visible=0, size=(1024, 768))\n",
    "virtual_display.start()\n",
    "\n",
    "# Other added\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sapien/_vulkan_tricks.py:21: UserWarning: Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\n",
      "  warn(\"Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\")\n",
      "/usr/local/lib/python3.10/dist-packages/sapien/_vulkan_tricks.py:37: UserWarning: Failed to find Vulkan ICD file. This is probably due to an incorrect or partial installation of the NVIDIA driver. SAPIEN will attempt to provide an ICD file anyway but it may not work.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, Union\n",
    "\n",
    "import numpy as np\n",
    "import sapien\n",
    "import torch\n",
    "import torch.random\n",
    "from transforms3d.euler import euler2quat\n",
    "\n",
    "from mani_skill.agents.robots import Fetch, Panda\n",
    "from mani_skill.envs.sapien_env import BaseEnv\n",
    "from mani_skill.sensors.camera import CameraConfig\n",
    "from mani_skill.utils import common, sapien_utils\n",
    "from mani_skill.utils.building import actors\n",
    "from mani_skill.utils.registration import register_env\n",
    "from mani_skill.utils.scene_builder.table import TableSceneBuilder\n",
    "from mani_skill.utils.structs import Pose\n",
    "from mani_skill.utils.structs.types import Array, GPUMemoryConfig, SimConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m2025-01-15 07:52:51,349 - mani_skill  - WARNING - Env PushCube-v1 is already registered. Skip registration.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "@register_env(\"PushCube-v1\", max_episode_steps=50)\n",
    "class PushCubeEnv(BaseEnv):\n",
    "\n",
    "    SUPPORTED_ROBOTS = [\"panda\",\"fetch\"]\n",
    "\n",
    "    agent: Union[Panda,Fetch]\n",
    "    goal_radius = 0.1\n",
    "    cube_half_size = 0.02\n",
    "\n",
    "    def __init__(self,*args,robot_uids=\"panda\",robot_init_qpos_noise=0.02,**kwargs):\n",
    "        self.robot_init_qpos_noise = robot_init_qpos_noise\n",
    "        super().__init__(*args,robot_uids=robot_uids,**kwargs) \n",
    "\n",
    "    def _load_scene(self,options:dict):\n",
    "        self.table_scene = TableSceneBuilder(\n",
    "            env=self,robot_init_qpos_noise=self.robot_init_qpos_noise\n",
    "        )\n",
    "        self.table_scene.build() # what does this line do?\n",
    "\n",
    "\n",
    "        self.obj = actors.build_cube(\n",
    "            self.scene,\n",
    "            half_size = self.cube_half_size,\n",
    "            color = np.array([12,42,160,255])/255,\n",
    "            name=\"cube\",\n",
    "            body_type=\"dynamic\",\n",
    "            initial_pose=sapien.Pose(p=[0,0,self.cube_half_size])\n",
    "        )\n",
    "\n",
    "        self.goal_region = actors.build_red_white_target(\n",
    "            self.scene,\n",
    "            radius = self.goal_radius,\n",
    "            thickness = 1e-5,\n",
    "            name=\"goal_region\",\n",
    "            add_collision=False,\n",
    "            body_type=\"kinematic\",\n",
    "            initial_pose=sapien.Pose(p=[0,0,1e-3])\n",
    "        )\n",
    "\n",
    "    def _load_agent(self,options:dict):\n",
    "        super()._load_agent(options,sapien.Pose(p=[-0.615,0,0]))\n",
    "\n",
    "    def _initialize_episode(self,env_idx:torch.Tensor,options:dict):\n",
    "        with torch.device(self.device):\n",
    "            # why would you only want to reinitialize some of the envs\n",
    "            b = len(env_idx)\n",
    "            self.table_scene.initialize(env_idx)\n",
    "\n",
    "            # xy coordinates are randomized and z is set to half_size\n",
    "            xyz = torch.zeros((b,3))\n",
    "            xyz[...,:2] = torch.rand((b,2)) * 0.2 - 0.1\n",
    "            xyz[...,2] = self.cube_half_size\n",
    "            q = [1,0,0,0]\n",
    "\n",
    "            obj_pose = Pose.create_from_pq(p=xyz,q=q)\n",
    "            self.obj.set_pose(obj_pose)\n",
    "\n",
    "            # some weird math positioning\n",
    "            # need to read up on why quarternions are better for representing rotations\n",
    "            target_region_xyz = xyz + torch.tensor([0.1 + self.goal_radius,0,0])\n",
    "            target_region_xyz[...,2] = 1e-3\n",
    "            self.goal_region.set_pose(\n",
    "                Pose.create_from_pq(\n",
    "                    p=target_region_xyz,\n",
    "                    q=euler2quat(0,np.pi/2,0)\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        is_obj_placed = (\n",
    "            torch.linalg.norm(\n",
    "                self.obj.pose.p[...,:2] - self.goal_region.pose.p[...,:2],axis=1\n",
    "            ) < self.goal_radius\n",
    "        )\n",
    "\n",
    "        # implementing fail condition\n",
    "        is_failed = (\n",
    "            self.obj.pose.p[...,2]<1\n",
    "        )\n",
    "    \n",
    "\n",
    "        return {\n",
    "            \"success\": is_obj_placed,\n",
    "            \"fail\": is_failed,\n",
    "            }\n",
    "\n",
    "    def _get_obs_extra(self,info:Dict):\n",
    "        obs = dict(\n",
    "            tcp_pose=self.agent.tcp.pose.raw_pose,\n",
    "        )\n",
    "        if self.obs_mode in [\"state\",\"state_dict\"]:\n",
    "            obs.update(\n",
    "                goal_pos = self.goal_region.pose.p,\n",
    "                obj_pose = self.obj.pose.raw_pose,\n",
    "            )\n",
    "        return obs\n",
    "\n",
    "    def compute_dense_reward(self, obs: Any, action: Array, info: Dict):\n",
    "        # We also create a pose marking where the robot should push the cube from that is easiest (pushing from behind the cube)\n",
    "        tcp_push_pose = Pose.create_from_pq(\n",
    "            p=self.obj.pose.p\n",
    "            + torch.tensor([-self.cube_half_size - 0.005, 0, 0], device=self.device)\n",
    "        )\n",
    "        tcp_to_push_pose = tcp_push_pose.p - self.agent.tcp.pose.p\n",
    "        tcp_to_push_pose_dist = torch.linalg.norm(tcp_to_push_pose, axis=1)\n",
    "        reaching_reward = 1 - torch.tanh(5 * tcp_to_push_pose_dist)\n",
    "        reward = reaching_reward\n",
    "\n",
    "        # compute a placement reward to encourage robot to move the cube to the center of the goal region\n",
    "        # we further multiply the place_reward by a mask reached so we only add the place reward if the robot has reached the desired push pose\n",
    "        # This reward design helps train RL agents faster by staging the reward out.\n",
    "        reached = tcp_to_push_pose_dist < 0.01\n",
    "        obj_to_goal_dist = torch.linalg.norm(\n",
    "            self.obj.pose.p[..., :2] - self.goal_region.pose.p[..., :2], axis=1\n",
    "        )\n",
    "        place_reward = 1 - torch.tanh(5 * obj_to_goal_dist)\n",
    "        reward += place_reward * reached\n",
    "\n",
    "        # assign rewards to parallel environments that achieved success to the maximum of 3.\n",
    "        reward[info[\"success\"]] = 3\n",
    "        return reward\n",
    "\n",
    "    def compute_normalized_dense_reward(self, obs: Any, action: Array, info: Dict):\n",
    "        # this should be equal to compute_dense_reward / max possible reward\n",
    "        max_reward = 3.0\n",
    "        return self.compute_dense_reward(obs=obs, action=action, info=info) / max_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with reward: tensor([0.1192])\n",
      "Success: tensor([False]), Failed: tensor([True])\n",
      "Episode 2 finished with reward: tensor([0.0981])\n",
      "Success: tensor([False]), Failed: tensor([True])\n",
      "Episode 3 finished with reward: tensor([0.1181])\n",
      "Success: tensor([False]), Failed: tensor([True])\n"
     ]
    }
   ],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()\n",
    "\n",
    "env = PushCubeEnv()\n",
    "env.reset()\n",
    "env.render_mode = \"rgb_array\"\n",
    "\n",
    "\n",
    "for episode in range(3):\n",
    "    obs = env.reset()\n",
    "    ep_reward = 0\n",
    "    \n",
    "    for step in range(50):\n",
    "        action = env.action_space.sample()\n",
    "        obs,reward,terminated,truncated,info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        if terminated:\n",
    "            print(f\"Episode {episode + 1} finished with reward: {ep_reward}\")\n",
    "            print(f\"Success: {info['success']}, Failed: {info['fail']}\")\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
