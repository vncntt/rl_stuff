{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoxOjIlOImwx"
      },
      "source": [
        "# Stable Baselines3 Tutorial - Creating a custom Gym environment\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "SB3-Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn how to use your own environment following the OpenAI Gym interface.\n",
        "Once it is done, you can easily use any compatible (depending on the action space) RL algorithm from Stable Baselines on that environment.\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fuDQ3DwmluoA"
      },
      "outputs": [],
      "source": [
        "# for autoformatting\n",
        "# %load_ext jupyter_black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Sp8rSS4DIhEV",
        "outputId": "0c9a76ce-9ec9-413c-b07b-1eb85bc1011b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable-baselines3>=2.0.0a4 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.5.0a0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.5.1)\n",
            "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.2.1)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.8.4)\n",
            "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.18.0)\n",
            "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (4.66.4)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (13.3.5)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (0.10.1)\n",
            "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (10.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.67.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.4.1)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (5.28.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (69.5.1)\n",
            "Requirement already satisfied: six>1.9 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.0.3)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.13.1)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2024.3.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2023.3)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a4) (0.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\"\n",
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzevZcgmJmhi"
      },
      "source": [
        "## First steps with the gym interface\n",
        "\n",
        "As you have noticed in the previous notebooks, an environment that follows the gym interface is quite simple to use.\n",
        "It provides to this user mainly three methods, which have the following signature (for gym versions > 0.26)\n",
        "- `reset()` called at the beginning of an episode, it returns an observation and a dictionary with additional info (defaults to an empty dict)\n",
        "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether new state is a terminal state (episode is finished), whether the max number of timesteps is reached (episode is artificially finished), and additional information\n",
        "- (Optional) `render()` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `render_mode='rbg_array'` to retrieve an image of the scene).\n",
        "\n",
        "Under the hood, it also contains two useful properties:\n",
        "- `observation_space` which one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
        "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
        "\n",
        "The best way to learn about [gym spaces](https://gymnasium.farama.org/api/spaces/) is to look at the [source code](https://github.com/Farama-Foundation/Gymnasium/tree/main/gymnasium/spaces), but you need to know at least the main ones:\n",
        "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of [a, b], (-oo, b], [a, oo), or (-oo, oo). Example: A 1D-Vector or an image observation can be described with the Box space.\n",
        "```python\n",
        "# Example for using image as input:\n",
        "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
        "```                                       \n",
        "\n",
        "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
        "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1.\n",
        "\n",
        "\n",
        "[Documentation on custom env](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)\n",
        "\n",
        "Also keep in mind that Stabe-baselines internally uses the previous gym API (<0.26), so every VecEnv returns only the observation after resetting and returns a 4-tuple instead of a 5-tuple  (terminated & truncated are already combined to done)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I98IKKyNJl6K",
        "outputId": "b8f464a5-1755-4f04-f820-e7f59c9d5b81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Shape: (4,)\n",
            "Action space: Discrete(2)\n",
            "Sampled action: 0\n",
            "(4,) 1.0 False False {}\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Box(4,) means that it is a Vector with 4 components\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Shape:\", env.observation_space.shape)\n",
        "# Discrete(2) means that there is two discrete actions\n",
        "print(\"Action space:\", env.action_space)\n",
        "\n",
        "# The reset method is called at the beginning of an episode\n",
        "obs, info = env.reset()\n",
        "# Sample a random action\n",
        "action = env.action_space.sample()\n",
        "print(\"Sampled action:\", action)\n",
        "obs, reward, terminated, truncated, info = env.step(action)\n",
        "# Note the obs is a numpy array\n",
        "# info is an empty dict for now but can contain any debugging info\n",
        "# reward is a scalar\n",
        "print(obs.shape, reward, terminated, truncated, info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqxatIwPOXe_"
      },
      "source": [
        "##  Gym env skeleton\n",
        "\n",
        "In practice this is how a gym environment looks like.\n",
        "Here, we have implemented a simple grid world were the agent must learn to go always left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rYzDXA9vJfz1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "\n",
        "class GoLeftEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Environment that follows gym interface.\n",
        "    This is a simple env where the agent must learn to go always left.\n",
        "    \"\"\"\n",
        "\n",
        "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "    metadata = {\"render_modes\": [\"console\"]}\n",
        "\n",
        "    # Define constants for clearer code\n",
        "    LEFT = 0\n",
        "    RIGHT = 1\n",
        "\n",
        "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
        "        super(GoLeftEnv, self).__init__()\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        # Size of the 1D-grid\n",
        "        self.grid_size = grid_size\n",
        "        # Initialize the agent at the right of the grid\n",
        "        self.agent_pos = grid_size - 1\n",
        "\n",
        "        # Define action and observation space\n",
        "        # They must be gym.spaces objects\n",
        "        # Example when using discrete actions, we have two: left and right\n",
        "        n_actions = 2\n",
        "        self.action_space = spaces.Discrete(n_actions)\n",
        "        # The observation will be the coordinate of the agent\n",
        "        # this can be described both by Discrete and Box space\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=self.grid_size, shape=(1,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Important: the observation must be a numpy array\n",
        "        :return: (np.array)\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed, options=options)\n",
        "        # Initialize the agent at the right of the grid\n",
        "        self.agent_pos = self.grid_size - 1\n",
        "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "        return np.array([self.agent_pos]).astype(np.float32), {}  # empty info dict\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == self.LEFT:\n",
        "            self.agent_pos -= 1\n",
        "        elif action == self.RIGHT:\n",
        "            self.agent_pos += 1\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Received invalid action={action} which is not part of the action space\"\n",
        "            )\n",
        "\n",
        "        # Account for the boundaries of the grid\n",
        "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
        "\n",
        "        # Are we at the left of the grid?\n",
        "        terminated = bool(self.agent_pos == 0)\n",
        "        truncated = False  # we do not limit the number of steps here\n",
        "\n",
        "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "        reward = 1 if self.agent_pos == 0 else 0\n",
        "\n",
        "        # Optionally we can pass additional info, we are not using that for now\n",
        "        info = {}\n",
        "\n",
        "        return (\n",
        "            np.array([self.agent_pos]).astype(np.float32),\n",
        "            reward,\n",
        "            terminated,\n",
        "            truncated,\n",
        "            info,\n",
        "        )\n",
        "\n",
        "    def render(self):\n",
        "        # agent is represented as a cross, rest as a dot\n",
        "        if self.render_mode == \"console\":\n",
        "            print(\".\" * self.agent_pos, end=\"\")\n",
        "            print(\"x\", end=\"\")\n",
        "            print(\".\" * (self.grid_size - self.agent_pos))\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy5mlho1-Ine"
      },
      "source": [
        "### Validate the environment\n",
        "\n",
        "Stable Baselines3 provides a [helper](https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html) to check that your environment follows the Gym interface. It also optionally checks that the environment is compatible with Stable-Baselines (and emits warning if necessary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9DOpP_B0-LXm"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.env_checker import check_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1CcUVatq-P0l"
      },
      "outputs": [],
      "source": [
        "env = GoLeftEnv()\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3khFtkSE0g"
      },
      "source": [
        "### Testing the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i62yf2LvSAYY",
        "outputId": "d2be0861-9353-4ec5-d1c4-b67f10a164e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".........x.\n",
            "Box(0.0, 10.0, (1,), float32)\n",
            "Discrete(2)\n",
            "1\n",
            "Step 1\n",
            "obs= [8.] reward= 0 done= False\n",
            "........x..\n",
            "Step 2\n",
            "obs= [7.] reward= 0 done= False\n",
            ".......x...\n",
            "Step 3\n",
            "obs= [6.] reward= 0 done= False\n",
            "......x....\n",
            "Step 4\n",
            "obs= [5.] reward= 0 done= False\n",
            ".....x.....\n",
            "Step 5\n",
            "obs= [4.] reward= 0 done= False\n",
            "....x......\n",
            "Step 6\n",
            "obs= [3.] reward= 0 done= False\n",
            "...x.......\n",
            "Step 7\n",
            "obs= [2.] reward= 0 done= False\n",
            "..x........\n",
            "Step 8\n",
            "obs= [1.] reward= 0 done= False\n",
            ".x.........\n",
            "Step 9\n",
            "obs= [0.] reward= 1 done= True\n",
            "x..........\n",
            "Goal reached! reward= 1\n"
          ]
        }
      ],
      "source": [
        "env = GoLeftEnv(grid_size=10)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.sample())\n",
        "\n",
        "GO_LEFT = 0\n",
        "# Hardcoded best agent: always go left!\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "    print(f\"Step {step + 1}\")\n",
        "    obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
        "    done = terminated or truncated\n",
        "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
        "    env.render()\n",
        "    if done:\n",
        "        print(\"Goal reached!\", \"reward=\", reward)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv1e1qJETfHU"
      },
      "source": [
        "### Try it with Stable-Baselines\n",
        "\n",
        "Once your environment follow the gym interface, it is quite easy to plug in any algorithm from stable-baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PQfLBE28SNDr"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# Instantiate the env\n",
        "vec_env = make_vec_env(GoLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zRV4Q7FVUKB6",
        "outputId": "771991d4-1fca-4096-985b-70859ee73ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 14.9     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 2258     |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.29    |\n",
            "|    explained_variance | 0.53     |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | 0.0718   |\n",
            "|    value_loss         | 0.0177   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 12.1     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 2804     |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.124   |\n",
            "|    explained_variance | -3.34    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 0.00424  |\n",
            "|    value_loss         | 0.0212   |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 9.83     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 3100     |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0993  |\n",
            "|    explained_variance | 0.617    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | 0.00126  |\n",
            "|    value_loss         | 0.00175  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 9.32      |\n",
            "|    ep_rew_mean        | 1         |\n",
            "| time/                 |           |\n",
            "|    fps                | 3283      |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0159   |\n",
            "|    explained_variance | 0.792     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -1.89e-05 |\n",
            "|    value_loss         | 0.000127  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 9.16     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 3393     |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.029   |\n",
            "|    explained_variance | 0.783    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 0.000269 |\n",
            "|    value_loss         | 0.000732 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 9.1       |\n",
            "|    ep_rew_mean        | 1         |\n",
            "| time/                 |           |\n",
            "|    fps                | 3471      |\n",
            "|    iterations         | 600       |\n",
            "|    time_elapsed       | 0         |\n",
            "|    total_timesteps    | 3000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00663  |\n",
            "|    explained_variance | 0.94      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 599       |\n",
            "|    policy_loss        | -7.87e-06 |\n",
            "|    value_loss         | 0.00011   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 9.04      |\n",
            "|    ep_rew_mean        | 1         |\n",
            "| time/                 |           |\n",
            "|    fps                | 3466      |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0221   |\n",
            "|    explained_variance | -1.08     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -0.000119 |\n",
            "|    value_loss         | 0.00147   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 9.04      |\n",
            "|    ep_rew_mean        | 1         |\n",
            "| time/                 |           |\n",
            "|    fps                | 2950      |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0071   |\n",
            "|    explained_variance | 0.88      |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -1.02e-05 |\n",
            "|    value_loss         | 0.00015   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 9.02      |\n",
            "|    ep_rew_mean        | 1         |\n",
            "| time/                 |           |\n",
            "|    fps                | 2949      |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00227  |\n",
            "|    explained_variance | 0.914     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -2.45e-06 |\n",
            "|    value_loss         | 0.000134  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 9         |\n",
            "|    ep_rew_mean        | 1         |\n",
            "| time/                 |           |\n",
            "|    fps                | 2925      |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 1         |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00666  |\n",
            "|    explained_variance | 0.621     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -1.19e-05 |\n",
            "|    value_loss         | 0.000178  |\n",
            "-------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Train the agent\n",
        "model = A2C(\"MlpPolicy\", env, verbose=1).learn(5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJbeiF0RUN-p",
        "outputId": "4b3959e4-9df0-4365-fd86-f42adb0e02c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1\n",
            "Action:  [0]\n",
            "obs= [[8.]] reward= [0.] done= [False]\n",
            "........x..\n",
            "Step 2\n",
            "Action:  [0]\n",
            "obs= [[7.]] reward= [0.] done= [False]\n",
            ".......x...\n",
            "Step 3\n",
            "Action:  [0]\n",
            "obs= [[6.]] reward= [0.] done= [False]\n",
            "......x....\n",
            "Step 4\n",
            "Action:  [0]\n",
            "obs= [[5.]] reward= [0.] done= [False]\n",
            ".....x.....\n",
            "Step 5\n",
            "Action:  [0]\n",
            "obs= [[4.]] reward= [0.] done= [False]\n",
            "....x......\n",
            "Step 6\n",
            "Action:  [0]\n",
            "obs= [[3.]] reward= [0.] done= [False]\n",
            "...x.......\n",
            "Step 7\n",
            "Action:  [0]\n",
            "obs= [[2.]] reward= [0.] done= [False]\n",
            "..x........\n",
            "Step 8\n",
            "Action:  [0]\n",
            "obs= [[1.]] reward= [0.] done= [False]\n",
            ".x.........\n",
            "Step 9\n",
            "Action:  [0]\n",
            "obs= [[9.]] reward= [1.] done= [ True]\n",
            ".........x.\n",
            "Goal reached! reward= [1.]\n"
          ]
        }
      ],
      "source": [
        "# Test the trained agent\n",
        "# using the vecenv\n",
        "obs = vec_env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    print(f\"Step {step + 1}\")\n",
        "    print(\"Action: \", action)\n",
        "    obs, reward, done, info = vec_env.step(action)\n",
        "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
        "    vec_env.render()\n",
        "    if done:\n",
        "        # Note that the VecEnv resets automatically\n",
        "        # when a done signal is encountered\n",
        "        print(\"Goal reached!\", \"reward=\", reward)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOggIa9sU--b"
      },
      "source": [
        "## It is your turn now, be creative!\n",
        "\n",
        "As an exercise, that's now your turn to build a custom gym environment.\n",
        "There is no constrain about what to do, be creative! (but not too creative, there is not enough time for that)\n",
        "\n",
        "If you don't have any idea, here is is a list of the environment you can implement:\n",
        "- Transform the discrete grid world to a continuous one, you will need to change a bit the logic and the action space\n",
        "- Create a 2D grid world and add walls\n",
        "- Create a tic-tac-toe game\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBDp4Pm-Uh4D",
        "outputId": "30daaba8-e809-4de3-8111-5cff0785d66c"
      },
      "outputs": [],
      "source": [
        "# Tic Tac Toe environment\n",
        "\n",
        "class TicTacToeEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Environment that follows gym interface.\n",
        "    This is a simple env where the agent must learn to play tic tac toe\n",
        "    \"\"\"\n",
        "\n",
        "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "    metadata = {\"render_modes\": [\"console\"]}\n",
        "    AGENT_TURN = 1\n",
        "    RANDOM_TURN = 2\n",
        "\n",
        "    def __init__(self, grid_size=3, render_mode=\"console\"):\n",
        "        super(TicTacToeEnv, self).__init__()\n",
        "        self.render_mode = render_mode\n",
        "        self.turn = np.random.choice([self.AGENT_TURN, self.RANDOM_TURN])\n",
        "        self.grid_size = grid_size\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=2, shape=(grid_size * grid_size,), dtype=np.int8 #flattened board instead of nxn\n",
        "        )\n",
        "        self.board = np.zeros((grid_size * grid_size,), dtype=np.int8)\n",
        "        # action space is picking one of the boxes\n",
        "        self.action_space = spaces.Discrete(grid_size * grid_size)\n",
        "\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Important: the observation must be a numpy array\n",
        "        :return: (np.array)\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed, options=options)\n",
        "        self.board = np.zeros((self.grid_size*self.grid_size,), dtype=np.int8)\n",
        "        self.turn = np.random.choice([self.AGENT_TURN, self.RANDOM_TURN]) #doesn't do shit right now\n",
        "        return np.array(self.board),{}\n",
        "\n",
        "    def random_step(self):\n",
        "        valid_moves = np.where(self.board == 0)[0]\n",
        "        rand_action = np.random.choice(valid_moves)\n",
        "        # random step move\n",
        "        self.board[rand_action] = 2\n",
        "        board2d = self.board.reshape((self.grid_size,self.grid_size))\n",
        "        truncated = bool(self.board.flatten().all()) # truncated if all cells are filled\n",
        "\n",
        "        if (np.any(np.all(board2d == 2, axis=1)) or  # rows\n",
        "            np.any(np.all(board2d == 2, axis=0)) or  # columns\n",
        "            np.all(np.diag(board2d) == 2) or         # diagonal\n",
        "            np.all(np.diag(np.fliplr(board2d)) == 2)): # anti-diagonal\n",
        "            return np.array(self.board), -1, True, False, {}\n",
        "        # ----------\n",
        "        return np.array(self.board), 0, False, truncated, {}\n",
        "\n",
        "    def agent_step(self,action):\n",
        "        if (self.board[action]!=0):\n",
        "            return(\n",
        "                True,\n",
        "                self.board,\n",
        "                -3,\n",
        "                True, # end the episode if an invalid action is made\n",
        "                False,\n",
        "                {\"invalid_action\":True}\n",
        "            )\n",
        "        #Agent makes moves\n",
        "        self.board[action] = 1 \n",
        "        # agent action over\n",
        "        board2d = self.board.reshape((self.grid_size,self.grid_size))\n",
        "        if (np.any(np.all(board2d == 1, axis=1)) or  # rows\n",
        "            np.any(np.all(board2d == 1, axis=0)) or  # columns\n",
        "            np.all(np.diag(board2d) == 1) or         # diagonal\n",
        "            np.all(np.diag(np.fliplr(board2d)) == 1)): # anti-diagonal\n",
        "            return True, np.array(self.board), 1, True, False, {}\n",
        "\n",
        "        reward = 0\n",
        "        truncated = bool(self.board.flatten().all()) # truncated if all cells are filled\n",
        "        terminated = False\n",
        "        # check if the game is over\n",
        "        if truncated:\n",
        "            return True, np.array(self.board), reward, terminated, truncated, {}\n",
        "\n",
        "        # first element False: doesn't want step function to return anything\n",
        "        return False, np.array(self.board), reward, terminated, truncated, {}\n",
        "\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        # such a horrible solution\n",
        "        # deal with first turn randomness\n",
        "        # let random agent act first half the time\n",
        "        if self.turn == self.RANDOM_TURN:\n",
        "            self.turn = self.AGENT_TURN\n",
        "            return self.random_step(action)\n",
        "\n",
        "        return_val, board, reward, terminated, truncated ,info = self.agent_step(action)\n",
        "        if return_val:\n",
        "            return board, reward, terminated, truncated, info \n",
        "        \n",
        "        # RANDOM TURN\n",
        "        return self.random_step()\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        board2d = self.board.reshape((self.grid_size,self.grid_size))\n",
        "        for i in range(self.grid_size):\n",
        "            for j in range(self.grid_size):\n",
        "              if j == self.grid_size - 1:\n",
        "                  print(board2d[i,j])\n",
        "              else:\n",
        "                  print(board2d[i,j],end=\"|\")             \n",
        "            if i != self.grid_size - 1:\n",
        "                print(\"-\"*(2*self.grid_size-1))\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bqKyMKv8z_7y"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "TicTacToeEnv.random_step() takes 1 positional argument but 2 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m GRID_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m TicTacToeEnv(grid_size\u001b[38;5;241m=\u001b[39mGRID_SIZE)\n\u001b[0;32m----> 3\u001b[0m check_env(env,warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m make_vec_env(TicTacToeEnv, n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, env_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(grid_size\u001b[38;5;241m=\u001b[39mGRID_SIZE))\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/env_checker.py:481\u001b[0m, in \u001b[0;36mcheck_env\u001b[0;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# ============ Check the returned values ===============\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m _check_returned_values(env, observation_space, action_space)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# ==== Check the render method and the declared render modes ====\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_render_check:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/env_checker.py:312\u001b[0m, in \u001b[0;36m_check_returned_values\u001b[0;34m(env, observation_space, action_space)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# Sample a random action\u001b[39;00m\n\u001b[1;32m    311\u001b[0m action \u001b[38;5;241m=\u001b[39m action_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m--> 312\u001b[0m data \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m, (\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `step()` method must return five values: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs, reward, terminated, truncated, info. Actual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m values returned.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    317\u001b[0m )\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# Unpack\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[11], line 91\u001b[0m, in \u001b[0;36mTicTacToeEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mturn \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRANDOM_TURN:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mturn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAGENT_TURN\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_step(action)\n\u001b[1;32m     93\u001b[0m return_val, board, reward, terminated, truncated ,info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_step(action)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_val:\n",
            "\u001b[0;31mTypeError\u001b[0m: TicTacToeEnv.random_step() takes 1 positional argument but 2 were given"
          ]
        }
      ],
      "source": [
        "GRID_SIZE = 3\n",
        "env = TicTacToeEnv(grid_size=GRID_SIZE)\n",
        "check_env(env,warn=True)\n",
        "\n",
        "vec_env = make_vec_env(TicTacToeEnv, n_envs=1, env_kwargs=dict(grid_size=GRID_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.49     |\n",
            "|    ep_rew_mean     | -2.5     |\n",
            "| time/              |          |\n",
            "|    fps             | 6547     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 0        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.5         |\n",
            "|    ep_rew_mean          | -2.36       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 4837        |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 0           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016748298 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.19       |\n",
            "|    explained_variance   | -0.198      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.463       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0339     |\n",
            "|    value_loss           | 2.19        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.66        |\n",
            "|    ep_rew_mean          | -2.03       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 4429        |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 1           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015495773 |\n",
            "|    clip_fraction        | 0.135       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.16       |\n",
            "|    explained_variance   | -0.0224     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.506       |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0315     |\n",
            "|    value_loss           | 1.22        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 3.84         |\n",
            "|    ep_rew_mean          | -1.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 4165         |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 1            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0143419495 |\n",
            "|    clip_fraction        | 0.12         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.12        |\n",
            "|    explained_variance   | -0.0161      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.808        |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.0323      |\n",
            "|    value_loss           | 1.72         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.97        |\n",
            "|    ep_rew_mean          | -1.51       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 4088        |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 2           |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015683487 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.06       |\n",
            "|    explained_variance   | -0.0131     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.859       |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0392     |\n",
            "|    value_loss           | 1.98        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.23        |\n",
            "|    ep_rew_mean          | -1.21       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3848        |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 3           |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013737302 |\n",
            "|    clip_fraction        | 0.14        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.99       |\n",
            "|    explained_variance   | -0.00712    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.879       |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0368     |\n",
            "|    value_loss           | 1.99        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.07        |\n",
            "|    ep_rew_mean          | -1.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3782        |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 3           |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013240514 |\n",
            "|    clip_fraction        | 0.117       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.93       |\n",
            "|    explained_variance   | -0.00695    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.18        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0341     |\n",
            "|    value_loss           | 2.21        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 4.07         |\n",
            "|    ep_rew_mean          | -0.47        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 3744         |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0128440745 |\n",
            "|    clip_fraction        | 0.138        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.86        |\n",
            "|    explained_variance   | -0.0113      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.855        |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.0324      |\n",
            "|    value_loss           | 2.02         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4           |\n",
            "|    ep_rew_mean          | -0.69       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3730        |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012656143 |\n",
            "|    clip_fraction        | 0.135       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.81       |\n",
            "|    explained_variance   | -0.00363    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.984       |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0323     |\n",
            "|    value_loss           | 1.96        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.22        |\n",
            "|    ep_rew_mean          | -0.39       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3718        |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012371648 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.76       |\n",
            "|    explained_variance   | 0.0096      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.867       |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0346     |\n",
            "|    value_loss           | 1.83        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.26        |\n",
            "|    ep_rew_mean          | -0.27       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3733        |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013570161 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.7        |\n",
            "|    explained_variance   | 0.00412     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.717       |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0353     |\n",
            "|    value_loss           | 1.78        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 4.14       |\n",
            "|    ep_rew_mean          | -0.18      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 3741       |\n",
            "|    iterations           | 12         |\n",
            "|    time_elapsed         | 6          |\n",
            "|    total_timesteps      | 24576      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01273297 |\n",
            "|    clip_fraction        | 0.145      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.65      |\n",
            "|    explained_variance   | 0.0147     |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.795      |\n",
            "|    n_updates            | 110        |\n",
            "|    policy_gradient_loss | -0.0299    |\n",
            "|    value_loss           | 1.76       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.02        |\n",
            "|    ep_rew_mean          | -0.27       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3749        |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013722268 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.6        |\n",
            "|    explained_variance   | 0.035       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.753       |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0297     |\n",
            "|    value_loss           | 1.57        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.2         |\n",
            "|    ep_rew_mean          | -0.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3738        |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010280672 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.57       |\n",
            "|    explained_variance   | 0.039       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.815       |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0268     |\n",
            "|    value_loss           | 1.44        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.14        |\n",
            "|    ep_rew_mean          | 0.11        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3747        |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010997404 |\n",
            "|    clip_fraction        | 0.125       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.51       |\n",
            "|    explained_variance   | 0.0287      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.721       |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0259     |\n",
            "|    value_loss           | 1.32        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.14        |\n",
            "|    ep_rew_mean          | 0.17        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3755        |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011860207 |\n",
            "|    clip_fraction        | 0.144       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.48       |\n",
            "|    explained_variance   | 0.0404      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.449       |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0278     |\n",
            "|    value_loss           | 1.18        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.18        |\n",
            "|    ep_rew_mean          | 0.18        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3740        |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012103477 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.0535      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.421       |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0256     |\n",
            "|    value_loss           | 1.09        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.16        |\n",
            "|    ep_rew_mean          | 0.27        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3746        |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011596192 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0.049       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.534       |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0237     |\n",
            "|    value_loss           | 1           |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.15        |\n",
            "|    ep_rew_mean          | 0.42        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3749        |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012225196 |\n",
            "|    clip_fraction        | 0.122       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.31       |\n",
            "|    explained_variance   | 0.0375      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.58        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.0245     |\n",
            "|    value_loss           | 1.03        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 3.97       |\n",
            "|    ep_rew_mean          | 0.27       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 3751       |\n",
            "|    iterations           | 20         |\n",
            "|    time_elapsed         | 10         |\n",
            "|    total_timesteps      | 40960      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01385181 |\n",
            "|    clip_fraction        | 0.143      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.24      |\n",
            "|    explained_variance   | 0.0693     |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.486      |\n",
            "|    n_updates            | 190        |\n",
            "|    policy_gradient_loss | -0.0257    |\n",
            "|    value_loss           | 0.904      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.08        |\n",
            "|    ep_rew_mean          | 0.49        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3746        |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010409825 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.2        |\n",
            "|    explained_variance   | 0.0312      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.363       |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0234     |\n",
            "|    value_loss           | 0.966       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4           |\n",
            "|    ep_rew_mean          | 0.26        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3752        |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011801737 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.0681      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.397       |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.0278     |\n",
            "|    value_loss           | 1.02        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 4.11         |\n",
            "|    ep_rew_mean          | 0.33         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 3758         |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 47104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0096286945 |\n",
            "|    clip_fraction        | 0.117        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.14        |\n",
            "|    explained_variance   | 0.0592       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.557        |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.0238      |\n",
            "|    value_loss           | 0.944        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.03        |\n",
            "|    ep_rew_mean          | 0.16        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3762        |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009912677 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.0616      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.185       |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0196     |\n",
            "|    value_loss           | 0.838       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.1         |\n",
            "|    ep_rew_mean          | 0.35        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3755        |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010726062 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | 0.057       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.308       |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0229     |\n",
            "|    value_loss           | 0.847       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.12        |\n",
            "|    ep_rew_mean          | 0.44        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3758        |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011514094 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0.0607      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.404       |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.0209     |\n",
            "|    value_loss           | 0.835       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.02        |\n",
            "|    ep_rew_mean          | 0.38        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3764        |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008384034 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.95       |\n",
            "|    explained_variance   | 0.105       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.267       |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.0208     |\n",
            "|    value_loss           | 0.746       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.97        |\n",
            "|    ep_rew_mean          | 0.64        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3754        |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 15          |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010553708 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.887      |\n",
            "|    explained_variance   | 0.112       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.213       |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0214     |\n",
            "|    value_loss           | 0.607       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.07        |\n",
            "|    ep_rew_mean          | 0.51        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3759        |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 15          |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008899187 |\n",
            "|    clip_fraction        | 0.0923      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.887      |\n",
            "|    explained_variance   | 0.0815      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.226       |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.0178     |\n",
            "|    value_loss           | 0.765       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.97        |\n",
            "|    ep_rew_mean          | 0.57        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3750        |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 16          |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008741604 |\n",
            "|    clip_fraction        | 0.088       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.857      |\n",
            "|    explained_variance   | 0.0611      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.231       |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.0189     |\n",
            "|    value_loss           | 0.849       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.89        |\n",
            "|    ep_rew_mean          | 0.44        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3737        |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 16          |\n",
            "|    total_timesteps      | 63488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008369034 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.84       |\n",
            "|    explained_variance   | 0.0829      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.204       |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.0186     |\n",
            "|    value_loss           | 0.592       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.95        |\n",
            "|    ep_rew_mean          | 0.75        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3726        |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009622198 |\n",
            "|    clip_fraction        | 0.094       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.862      |\n",
            "|    explained_variance   | 0.0663      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.493       |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.0194     |\n",
            "|    value_loss           | 0.856       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.94        |\n",
            "|    ep_rew_mean          | 0.65        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3729        |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009701653 |\n",
            "|    clip_fraction        | 0.0849      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.837      |\n",
            "|    explained_variance   | 0.0485      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.151       |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.018      |\n",
            "|    value_loss           | 0.618       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.97        |\n",
            "|    ep_rew_mean          | 0.61        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3731        |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 69632       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010468101 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.82       |\n",
            "|    explained_variance   | 0.0564      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.284       |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | -0.0193     |\n",
            "|    value_loss           | 0.623       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.99        |\n",
            "|    ep_rew_mean          | 0.61        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3721        |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 19          |\n",
            "|    total_timesteps      | 71680       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009932658 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.788      |\n",
            "|    explained_variance   | 0.0726      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.331       |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | -0.019      |\n",
            "|    value_loss           | 0.577       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.85        |\n",
            "|    ep_rew_mean          | 0.58        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3719        |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 19          |\n",
            "|    total_timesteps      | 73728       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008926311 |\n",
            "|    clip_fraction        | 0.0947      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.78       |\n",
            "|    explained_variance   | 0.122       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.373       |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | -0.0154     |\n",
            "|    value_loss           | 0.493       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.11        |\n",
            "|    ep_rew_mean          | 0.62        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3720        |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 20          |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008112893 |\n",
            "|    clip_fraction        | 0.0873      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.779      |\n",
            "|    explained_variance   | 0.108       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.191       |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.0161     |\n",
            "|    value_loss           | 0.5         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.88        |\n",
            "|    ep_rew_mean          | 0.67        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3718        |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 20          |\n",
            "|    total_timesteps      | 77824       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012053552 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.765      |\n",
            "|    explained_variance   | 0.102       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.371       |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.0203     |\n",
            "|    value_loss           | 0.57        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.03        |\n",
            "|    ep_rew_mean          | 0.54        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3714        |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 21          |\n",
            "|    total_timesteps      | 79872       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009662323 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.755      |\n",
            "|    explained_variance   | 0.119       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.272       |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.0187     |\n",
            "|    value_loss           | 0.548       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 3.92         |\n",
            "|    ep_rew_mean          | 0.58         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 3721         |\n",
            "|    iterations           | 40           |\n",
            "|    time_elapsed         | 22           |\n",
            "|    total_timesteps      | 81920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071377773 |\n",
            "|    clip_fraction        | 0.0873       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.764       |\n",
            "|    explained_variance   | 0.142        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.144        |\n",
            "|    n_updates            | 390          |\n",
            "|    policy_gradient_loss | -0.0165      |\n",
            "|    value_loss           | 0.552        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.98        |\n",
            "|    ep_rew_mean          | 0.77        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3718        |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 22          |\n",
            "|    total_timesteps      | 83968       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011508966 |\n",
            "|    clip_fraction        | 0.123       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.74       |\n",
            "|    explained_variance   | 0.101       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.324       |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | -0.0234     |\n",
            "|    value_loss           | 0.617       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.96        |\n",
            "|    ep_rew_mean          | 0.53        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3723        |\n",
            "|    iterations           | 42          |\n",
            "|    time_elapsed         | 23          |\n",
            "|    total_timesteps      | 86016       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009436054 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.727      |\n",
            "|    explained_variance   | 0.09        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.233       |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.0167     |\n",
            "|    value_loss           | 0.406       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.99        |\n",
            "|    ep_rew_mean          | 0.54        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3717        |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 23          |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011136174 |\n",
            "|    clip_fraction        | 0.125       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.718      |\n",
            "|    explained_variance   | 0.129       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.188       |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.0187     |\n",
            "|    value_loss           | 0.505       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 3.94       |\n",
            "|    ep_rew_mean          | 0.55       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 3722       |\n",
            "|    iterations           | 44         |\n",
            "|    time_elapsed         | 24         |\n",
            "|    total_timesteps      | 90112      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01159339 |\n",
            "|    clip_fraction        | 0.135      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.717     |\n",
            "|    explained_variance   | 0.128      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.14       |\n",
            "|    n_updates            | 430        |\n",
            "|    policy_gradient_loss | -0.0197    |\n",
            "|    value_loss           | 0.476      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.91        |\n",
            "|    ep_rew_mean          | 0.62        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3726        |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 24          |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008809611 |\n",
            "|    clip_fraction        | 0.0783      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.713      |\n",
            "|    explained_variance   | 0.102       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.338       |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.0162     |\n",
            "|    value_loss           | 0.546       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 3.93      |\n",
            "|    ep_rew_mean          | 0.69      |\n",
            "| time/                   |           |\n",
            "|    fps                  | 3723      |\n",
            "|    iterations           | 46        |\n",
            "|    time_elapsed         | 25        |\n",
            "|    total_timesteps      | 94208     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0106056 |\n",
            "|    clip_fraction        | 0.119     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.65     |\n",
            "|    explained_variance   | 0.112     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.224     |\n",
            "|    n_updates            | 450       |\n",
            "|    policy_gradient_loss | -0.0213   |\n",
            "|    value_loss           | 0.396     |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.91        |\n",
            "|    ep_rew_mean          | 0.67        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3726        |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 25          |\n",
            "|    total_timesteps      | 96256       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008791332 |\n",
            "|    clip_fraction        | 0.0988      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.635      |\n",
            "|    explained_variance   | 0.152       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.103       |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.0163     |\n",
            "|    value_loss           | 0.462       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 3.95      |\n",
            "|    ep_rew_mean          | 0.77      |\n",
            "| time/                   |           |\n",
            "|    fps                  | 3726      |\n",
            "|    iterations           | 48        |\n",
            "|    time_elapsed         | 26        |\n",
            "|    total_timesteps      | 98304     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0066606 |\n",
            "|    clip_fraction        | 0.0898    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.645    |\n",
            "|    explained_variance   | 0.114     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.275     |\n",
            "|    n_updates            | 470       |\n",
            "|    policy_gradient_loss | -0.0163   |\n",
            "|    value_loss           | 0.418     |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.89        |\n",
            "|    ep_rew_mean          | 0.83        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3727        |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 26          |\n",
            "|    total_timesteps      | 100352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008640661 |\n",
            "|    clip_fraction        | 0.0911      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.645      |\n",
            "|    explained_variance   | 0.103       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.292       |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | -0.0155     |\n",
            "|    value_loss           | 0.417       |\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = PPO(\"MlpPolicy\", env, verbose=1).learn(100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0|0|0\n",
            "-----\n",
            "0|1|0\n",
            "-----\n",
            "0|0|2\n",
            "reward= [0.] done [False]\n"
          ]
        }
      ],
      "source": [
        "obs = vec_env.reset()\n",
        "\n",
        "action , _ = model.predict(obs,deterministic=True)\n",
        "obs, reward, done, info = vec_env.step(action)\n",
        "vec_env.render()\n",
        "print(\"reward=\", reward, \"done\", done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0|0|1\n",
            "-----\n",
            "0|1|2\n",
            "-----\n",
            "0|0|2\n",
            "reward= [0.] done [False]\n"
          ]
        }
      ],
      "source": [
        "action , _ = model.predict(obs,deterministic=True)\n",
        "obs, reward, done, info = vec_env.step(action)\n",
        "vec_env.render()\n",
        "print(\"reward=\", reward, \"done\", done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0|0|0\n",
            "-----\n",
            "0|0|0\n",
            "-----\n",
            "0|0|0\n",
            "reward= [1.] done [ True]\n"
          ]
        }
      ],
      "source": [
        "action , _ = model.predict(obs,deterministic=True)\n",
        "obs, reward, done, info = vec_env.step(action)\n",
        "vec_env.render()\n",
        "print(\"reward=\", reward, \"done\", done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0|0|0\n",
            "-----\n",
            "0|0|0\n",
            "-----\n",
            "0|0|0\n",
            "reward= [1.] done [ True]\n"
          ]
        }
      ],
      "source": [
        "action , _ = model.predict(obs,deterministic=True)\n",
        "obs, reward, done, info = vec_env.step(action)\n",
        "vec_env.render()\n",
        "print(\"reward=\", reward, \"done\", done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2|0|0|0|2\n",
            "---------\n",
            "0|1|1|1|1\n",
            "---------\n",
            "0|1|0|2|0\n",
            "---------\n",
            "0|2|0|0|0\n",
            "---------\n",
            "0|0|0|2|0\n",
            "reward= [0.] done [False]\n"
          ]
        }
      ],
      "source": [
        "action , _ = model.predict(obs,deterministic=True)\n",
        "obs, reward, done, info = vec_env.step(action)\n",
        "vec_env.render()\n",
        "print(\"reward=\", reward, \"done\", done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0|0|0|0|0\n",
            "---------\n",
            "0|0|0|0|0\n",
            "---------\n",
            "0|0|0|0|0\n",
            "---------\n",
            "0|0|0|0|0\n",
            "---------\n",
            "0|0|0|0|0\n",
            "reward= [1.] done [ True]\n"
          ]
        }
      ],
      "source": [
        "action , _ = model.predict(obs,deterministic=True)\n",
        "obs, reward, done, info = vec_env.step(action)\n",
        "vec_env.render()\n",
        "print(\"reward=\", reward, \"done\", done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num wins for 3x3: 853\n"
          ]
        }
      ],
      "source": [
        "total_trials = 1000\n",
        "num_wins = 0\n",
        "for i in range(total_trials):\n",
        "    obs = vec_env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action , _ = model.predict(obs)\n",
        "        obs,reward,done,info = vec_env.step(action)\n",
        "    if reward == 1:\n",
        "        num_wins += 1\n",
        "print(\"num wins for 3x3:\", num_wins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0]], dtype=int8)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec_env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num wins: 744\n"
          ]
        }
      ],
      "source": [
        "# 5x5\n",
        "total_trials = 1000\n",
        "num_wins = 0\n",
        "for i in range(total_trials):\n",
        "    obs = vec_env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action , _ = model.predict(obs)\n",
        "        obs,reward,done,info = vec_env.step(action)\n",
        "    if reward == 1:\n",
        "        num_wins += 1\n",
        "print(\"num wins:\", num_wins)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "how is it even possible that my win rate is 75% but my average episode reward is 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num wins: 892\n"
          ]
        }
      ],
      "source": [
        "# 4x4\n",
        "total_trials = 1000\n",
        "num_wins = 0\n",
        "for i in range(total_trials):\n",
        "    obs = vec_env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action , _ = model.predict(obs)\n",
        "        obs,reward,done,info = vec_env.step(action)\n",
        "    if reward == 1:\n",
        "        num_wins += 1\n",
        "print(\"num wins:\", num_wins)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Can't debug because fucking vec_env resets when done. Bad because I can't see the last position\n",
        "- want to implement two agents playing together\n",
        "- current implementation of the env is probably wrong? embedding a random opponent into the env seems weird"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
