{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Sp8rSS4DIhEV",
        "outputId": "0c9a76ce-9ec9-413c-b07b-1eb85bc1011b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable-baselines3>=2.0.0a4 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.5.0a0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.5.1)\n",
            "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.2.1)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.8.4)\n",
            "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.18.0)\n",
            "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (4.66.4)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (13.3.5)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (0.10.1)\n",
            "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a4) (10.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.67.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.4.1)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (5.28.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (69.5.1)\n",
            "Requirement already satisfied: six>1.9 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.0.3)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.13.1)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2024.3.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2023.3)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a4) (0.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\"\n",
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzevZcgmJmhi"
      },
      "source": [
        "## First steps with the gym interface\n",
        "\n",
        "As you have noticed in the previous notebooks, an environment that follows the gym interface is quite simple to use.\n",
        "It provides to this user mainly three methods, which have the following signature (for gym versions > 0.26)\n",
        "- `reset()` called at the beginning of an episode, it returns an observation and a dictionary with additional info (defaults to an empty dict)\n",
        "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether new state is a terminal state (episode is finished), whether the max number of timesteps is reached (episode is artificially finished), and additional information\n",
        "- (Optional) `render()` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `render_mode='rbg_array'` to retrieve an image of the scene).\n",
        "\n",
        "Under the hood, it also contains two useful properties:\n",
        "- `observation_space` which one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
        "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
        "\n",
        "The best way to learn about [gym spaces](https://gymnasium.farama.org/api/spaces/) is to look at the [source code](https://github.com/Farama-Foundation/Gymnasium/tree/main/gymnasium/spaces), but you need to know at least the main ones:\n",
        "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of [a, b], (-oo, b], [a, oo), or (-oo, oo). Example: A 1D-Vector or an image observation can be described with the Box space.\n",
        "```python\n",
        "# Example for using image as input:\n",
        "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
        "```                                       \n",
        "\n",
        "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
        "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1.\n",
        "\n",
        "\n",
        "[Documentation on custom env](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)\n",
        "\n",
        "Also keep in mind that Stabe-baselines internally uses the previous gym API (<0.26), so every VecEnv returns only the observation after resetting and returns a 4-tuple instead of a 5-tuple  (terminated & truncated are already combined to done)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I98IKKyNJl6K",
        "outputId": "b8f464a5-1755-4f04-f820-e7f59c9d5b81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Shape: (4,)\n",
            "Action space: Discrete(2)\n",
            "Sampled action: 0\n",
            "(4,) 1.0 False False {}\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Box(4,) means that it is a Vector with 4 components\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Shape:\", env.observation_space.shape)\n",
        "# Discrete(2) means that there is two discrete actions\n",
        "print(\"Action space:\", env.action_space)\n",
        "\n",
        "# The reset method is called at the beginning of an episode\n",
        "obs, info = env.reset()\n",
        "# Sample a random action\n",
        "action = env.action_space.sample()\n",
        "print(\"Sampled action:\", action)\n",
        "obs, reward, terminated, truncated, info = env.step(action)\n",
        "# Note the obs is a numpy array\n",
        "# info is an empty dict for now but can contain any debugging info\n",
        "# reward is a scalar\n",
        "print(obs.shape, reward, terminated, truncated, info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqxatIwPOXe_"
      },
      "source": [
        "##  Gym env skeleton\n",
        "\n",
        "In practice this is how a gym environment looks like.\n",
        "Here, we have implemented a simple grid world were the agent must learn to go always left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rYzDXA9vJfz1"
      },
      "outputs": [],
      "source": [
        "class GoLeftEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Environment that follows gym interface.\n",
        "    This is a simple env where the agent must learn to go always left.\n",
        "    \"\"\"\n",
        "\n",
        "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "    metadata = {\"render_modes\": [\"console\"]}\n",
        "\n",
        "    # Define constants for clearer code\n",
        "    LEFT = 0\n",
        "    RIGHT = 1\n",
        "\n",
        "    def __init__(self, grid_size=10, render_mode=\"console\"):\n",
        "        super(GoLeftEnv, self).__init__()\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        # Size of the 1D-grid\n",
        "        self.grid_size = grid_size\n",
        "        # Initialize the agent at the right of the grid\n",
        "        self.agent_pos = grid_size - 1\n",
        "\n",
        "        # Define action and observation space\n",
        "        # They must be gym.spaces objects\n",
        "        # Example when using discrete actions, we have two: left and right\n",
        "        n_actions = 2\n",
        "        self.action_space = spaces.Discrete(n_actions)\n",
        "        # The observation will be the coordinate of the agent\n",
        "        # this can be described both by Discrete and Box space\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=self.grid_size, shape=(1,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Important: the observation must be a numpy array\n",
        "        :return: (np.array)\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed, options=options)\n",
        "        # Initialize the agent at the right of the grid\n",
        "        self.agent_pos = self.grid_size - 1\n",
        "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "        return np.array([self.agent_pos]).astype(np.float32), {}  # empty info dict\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == self.LEFT:\n",
        "            self.agent_pos -= 1\n",
        "        elif action == self.RIGHT:\n",
        "            self.agent_pos += 1\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Received invalid action={action} which is not part of the action space\"\n",
        "            )\n",
        "\n",
        "        # Account for the boundaries of the grid\n",
        "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
        "\n",
        "        # Are we at the left of the grid?\n",
        "        terminated = bool(self.agent_pos == 0)\n",
        "        truncated = False  # we do not limit the number of steps here\n",
        "\n",
        "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "        reward = 1 if self.agent_pos == 0 else 0\n",
        "\n",
        "        # Optionally we can pass additional info, we are not using that for now\n",
        "        info = {}\n",
        "\n",
        "        return (\n",
        "            np.array([self.agent_pos]).astype(np.float32),\n",
        "            reward,\n",
        "            terminated,\n",
        "            truncated,\n",
        "            info,\n",
        "        )\n",
        "\n",
        "    def render(self):\n",
        "        # agent is represented as a cross, rest as a dot\n",
        "        if self.render_mode == \"console\":\n",
        "            print(\".\" * self.agent_pos, end=\"\")\n",
        "            print(\"x\", end=\"\")\n",
        "            print(\".\" * (self.grid_size - self.agent_pos))\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy5mlho1-Ine"
      },
      "source": [
        "### Validate the environment\n",
        "\n",
        "Stable Baselines3 provides a [helper](https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html) to check that your environment follows the Gym interface. It also optionally checks that the environment is compatible with Stable-Baselines (and emits warning if necessary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1CcUVatq-P0l"
      },
      "outputs": [],
      "source": [
        "env = GoLeftEnv()\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3khFtkSE0g"
      },
      "source": [
        "### Testing the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i62yf2LvSAYY",
        "outputId": "d2be0861-9353-4ec5-d1c4-b67f10a164e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".........x.\n",
            "Box(0.0, 10.0, (1,), float32)\n",
            "Discrete(2)\n",
            "1\n",
            "Step 1\n",
            "obs= [8.] reward= 0 done= False\n",
            "........x..\n",
            "Step 2\n",
            "obs= [7.] reward= 0 done= False\n",
            ".......x...\n",
            "Step 3\n",
            "obs= [6.] reward= 0 done= False\n",
            "......x....\n",
            "Step 4\n",
            "obs= [5.] reward= 0 done= False\n",
            ".....x.....\n",
            "Step 5\n",
            "obs= [4.] reward= 0 done= False\n",
            "....x......\n",
            "Step 6\n",
            "obs= [3.] reward= 0 done= False\n",
            "...x.......\n",
            "Step 7\n",
            "obs= [2.] reward= 0 done= False\n",
            "..x........\n",
            "Step 8\n",
            "obs= [1.] reward= 0 done= False\n",
            ".x.........\n",
            "Step 9\n",
            "obs= [0.] reward= 1 done= True\n",
            "x..........\n",
            "Goal reached! reward= 1\n"
          ]
        }
      ],
      "source": [
        "env = GoLeftEnv(grid_size=10)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.sample())\n",
        "\n",
        "GO_LEFT = 0\n",
        "# Hardcoded best agent: always go left!\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "    print(f\"Step {step + 1}\")\n",
        "    obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
        "    done = terminated or truncated\n",
        "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
        "    env.render()\n",
        "    if done:\n",
        "        print(\"Goal reached!\", \"reward=\", reward)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv1e1qJETfHU"
      },
      "source": [
        "### Try it with Stable-Baselines\n",
        "\n",
        "Once your environment follow the gym interface, it is quite easy to plug in any algorithm from stable-baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PQfLBE28SNDr"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# Instantiate the env\n",
        "vec_env = make_vec_env(GoLeftEnv, n_envs=1, env_kwargs=dict(grid_size=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zRV4Q7FVUKB6",
        "outputId": "771991d4-1fca-4096-985b-70859ee73ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 15.9     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 154      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.413   |\n",
            "|    explained_variance | -1.99    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | 0.00263  |\n",
            "|    value_loss         | 0.00344  |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 12.9     |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 228      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.216   |\n",
            "|    explained_variance | -11.2    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | -0.00275 |\n",
            "|    value_loss         | 0.00328  |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 10.1      |\n",
            "|    ep_rew_mean        | 1         |\n",
            "| time/                 |           |\n",
            "|    fps                | 270       |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.126    |\n",
            "|    explained_variance | -6.47     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -0.000396 |\n",
            "|    value_loss         | 0.00117   |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 9.2      |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 298      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0315  |\n",
            "|    explained_variance | 0.817    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 0.000101 |\n",
            "|    value_loss         | 0.000502 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 9.12      |\n",
            "|    ep_rew_mean        | 1         |\n",
            "| time/                 |           |\n",
            "|    fps                | 319       |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 7         |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0256   |\n",
            "|    explained_variance | 0.878     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | -6.45e-05 |\n",
            "|    value_loss         | 0.000202  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 9.1      |\n",
            "|    ep_rew_mean        | 1        |\n",
            "| time/                 |          |\n",
            "|    fps                | 326      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.0145  |\n",
            "|    explained_variance | 0.997    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | 8.28e-06 |\n",
            "|    value_loss         | 1.65e-05 |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 9.06      |\n",
            "|    ep_rew_mean        | 1         |\n",
            "| time/                 |           |\n",
            "|    fps                | 325       |\n",
            "|    iterations         | 700       |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 3500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0355   |\n",
            "|    explained_variance | 0.974     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 699       |\n",
            "|    policy_loss        | -2.47e-05 |\n",
            "|    value_loss         | 3.21e-05  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 9.04      |\n",
            "|    ep_rew_mean        | 1         |\n",
            "| time/                 |           |\n",
            "|    fps                | 333       |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 11        |\n",
            "|    total_timesteps    | 4000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.0135   |\n",
            "|    explained_variance | 0.994     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | -5.63e-06 |\n",
            "|    value_loss         | 3.69e-06  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 9.04      |\n",
            "|    ep_rew_mean        | 1         |\n",
            "| time/                 |           |\n",
            "|    fps                | 343       |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00257  |\n",
            "|    explained_variance | 0.976     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | -1.46e-06 |\n",
            "|    value_loss         | 3.5e-05   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 9.02      |\n",
            "|    ep_rew_mean        | 1         |\n",
            "| time/                 |           |\n",
            "|    fps                | 350       |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.00664  |\n",
            "|    explained_variance | 0.869     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -8.61e-06 |\n",
            "|    value_loss         | 9.87e-05  |\n",
            "-------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Train the agent\n",
        "model = A2C(\"MlpPolicy\", env, verbose=1).learn(5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJbeiF0RUN-p",
        "outputId": "4b3959e4-9df0-4365-fd86-f42adb0e02c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1\n",
            "Action:  [0]\n",
            "obs= [[8.]] reward= [0.] done= [False]\n",
            "........x..\n",
            "Step 2\n",
            "Action:  [0]\n",
            "obs= [[7.]] reward= [0.] done= [False]\n",
            ".......x...\n",
            "Step 3\n",
            "Action:  [0]\n",
            "obs= [[6.]] reward= [0.] done= [False]\n",
            "......x....\n",
            "Step 4\n",
            "Action:  [0]\n",
            "obs= [[5.]] reward= [0.] done= [False]\n",
            ".....x.....\n",
            "Step 5\n",
            "Action:  [0]\n",
            "obs= [[4.]] reward= [0.] done= [False]\n",
            "....x......\n",
            "Step 6\n",
            "Action:  [0]\n",
            "obs= [[3.]] reward= [0.] done= [False]\n",
            "...x.......\n",
            "Step 7\n",
            "Action:  [0]\n",
            "obs= [[2.]] reward= [0.] done= [False]\n",
            "..x........\n",
            "Step 8\n",
            "Action:  [0]\n",
            "obs= [[1.]] reward= [0.] done= [False]\n",
            ".x.........\n",
            "Step 9\n",
            "Action:  [0]\n",
            "obs= [[9.]] reward= [1.] done= [ True]\n",
            ".........x.\n",
            "Goal reached! reward= [1.]\n"
          ]
        }
      ],
      "source": [
        "# Test the trained agent\n",
        "# using the vecenv\n",
        "obs = vec_env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    print(f\"Step {step + 1}\")\n",
        "    print(\"Action: \", action)\n",
        "    obs, reward, done, info = vec_env.step(action)\n",
        "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
        "    vec_env.render()\n",
        "    if done:\n",
        "        # Note that the VecEnv resets automatically\n",
        "        # when a done signal is encountered\n",
        "        print(\"Goal reached!\", \"reward=\", reward)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOggIa9sU--b"
      },
      "source": [
        "## It is your turn now, be creative!\n",
        "\n",
        "As an exercise, that's now your turn to build a custom gym environment.\n",
        "There is no constrain about what to do, be creative! (but not too creative, there is not enough time for that)\n",
        "\n",
        "If you don't have any idea, here is is a list of the environment you can implement:\n",
        "- Transform the discrete grid world to a continuous one, you will need to change a bit the logic and the action space\n",
        "- Create a 2D grid world and add walls\n",
        "- Create a tic-tac-toe game\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBDp4Pm-Uh4D",
        "outputId": "30daaba8-e809-4de3-8111-5cff0785d66c"
      },
      "outputs": [],
      "source": [
        "# Apple game environment\n",
        "\n",
        "class AppleGameEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Environment that follows gym interface.\n",
        "    This is a simple env where the agent must learn to play this addictive apple game \n",
        "    \"\"\"\n",
        "\n",
        "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "    metadata = {\"render_modes\": [\"console\"]}\n",
        "\n",
        "    def __init__(self, grid_size=3, render_mode=\"console\"):\n",
        "        super(AppleGameEnv, self).__init__()\n",
        "        self.render_mode = render_mode\n",
        "        self.width = 10\n",
        "        self.length = 10\n",
        "        self.num_turns = 1000  # maximum number of turns\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=9, shape=(self.width * self.length,), dtype=np.int8 #flattened board instead of nxn\n",
        "        )\n",
        "        self.board = np.zeros((grid_size * grid_size,), dtype=np.int8)\n",
        "        # action space is picking two corners of the grid\n",
        "        self.action_space = spaces.Box(low=np.array([1,1,1,1]), high=np.array([self.length,self.width,self.length,self.width]), dtype=np.int8)\n",
        "    \n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Important: the observation must be a numpy array\n",
        "        :return: (np.array)\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed, options=options)\n",
        "        self.board = np.random.randint(1,10,size=(self.width*self.length),dtype = np.int8)\n",
        "        return np.array(self.board),{}\n",
        "\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        self.num_turns -= 1\n",
        "        # extract the two points\n",
        "        x1,y1 = action[:2]\n",
        "        x2,y2 = action[2:]\n",
        "        top = min(y1,y2)\n",
        "        bottom = max(y1,y2)\n",
        "        left = min(x1,x2)\n",
        "        right = max(x1,x2)\n",
        "\n",
        "        # check if the sum of the points in the rectangle formed by the two corners is 10\n",
        "        # if 10, set all entries in the rectangle to 0\n",
        "        board_2d = self.board.reshape((self.length,self.width))\n",
        "        rectangle = board_2d[top-1:bottom,left-1:right]\n",
        "        sum = np.sum(rectangle)\n",
        "        reward = 0\n",
        "        if sum == 10:\n",
        "            board_2d[top-1:bottom,left-1:right] = 0\n",
        "            reward = (bottom-top)*(left-right)\n",
        "            self.board = board_2d.flatten()\n",
        "\n",
        "        terminated = self.num_turns <= 0\n",
        "        truncated = False\n",
        "        board_2d = self.board.reshape((self.length,self.width))\n",
        "        return (\n",
        "            np.array(self.board).astype(np.float32),\n",
        "            reward,\n",
        "            terminated,\n",
        "            truncated,\n",
        "            {}\n",
        "        ) \n",
        "\n",
        "    def render(self):\n",
        "        board2d = self.board.reshape((self.grid_size,self.grid_size))\n",
        "        for i in range(self.length):\n",
        "            for j in range(self.width):\n",
        "              if j == self.grid_size - 1:\n",
        "                  print(board2d[i,j])\n",
        "              else:\n",
        "                  print(board2d[i,j],end=\"|\")             \n",
        "            if i != self.grid_size - 1:\n",
        "                print(\"-\"*(2*self.width-1))\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3, 3, 7, 1], dtype=int8)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "action_space = spaces.Box(low=np.array([1,1,1,1]), high=np.array([10,10,10,10]), dtype=np.int8)\n",
        "action = action_space.sample()\n",
        "action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 3\n",
            "7 1\n",
            "1 3\n",
            "3 7\n"
          ]
        }
      ],
      "source": [
        "x1,y1 = action[:2]\n",
        "x2,y2 = action[2:]\n",
        "print(x1,y1)\n",
        "print(x2,y2)\n",
        "top = min(y1,y2)\n",
        "bottom = max(y1,y2)\n",
        "left = min(x1,x2)\n",
        "right = max(x1,x2)\n",
        "print(top,bottom)\n",
        "print(left,right)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[4, 1, 5, 4, 2, 7, 8, 4, 8, 1],\n",
              "       [3, 9, 4, 5, 2, 3, 1, 9, 5, 9],\n",
              "       [2, 3, 6, 2, 7, 9, 5, 6, 9, 7],\n",
              "       [3, 5, 7, 5, 8, 4, 4, 4, 7, 7],\n",
              "       [4, 5, 7, 6, 1, 7, 6, 5, 9, 2],\n",
              "       [4, 5, 3, 7, 2, 7, 4, 8, 5, 5],\n",
              "       [6, 8, 1, 6, 1, 3, 9, 4, 9, 3],\n",
              "       [1, 6, 9, 2, 1, 6, 7, 8, 2, 7],\n",
              "       [2, 4, 4, 7, 4, 4, 7, 2, 9, 4],\n",
              "       [5, 4, 7, 8, 8, 2, 8, 9, 5, 3]], dtype=int8)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "board = np.random.randint(1,10,size=(10,10),dtype=np.int8)\n",
        "board\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5, 7, 5, 8, 4, 4, 4,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "board[top,bottom]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "board[left,right]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[5, 4, 2, 7, 8],\n",
              "       [4, 5, 2, 3, 1],\n",
              "       [6, 2, 7, 9, 5]], dtype=int8)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rectangle = board[top-1:bottom,left-1:right] # [0:3]\n",
        "rectangle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bqKyMKv8z_7y"
      },
      "outputs": [],
      "source": [
        "GRID_SIZE = 3\n",
        "env = TicTacToeEnv(grid_size=GRID_SIZE)\n",
        "check_env(env,warn=True)\n",
        "\n",
        "vec_env = make_vec_env(TicTacToeEnv, n_envs=1, env_kwargs=dict(grid_size=GRID_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.49     |\n",
            "|    ep_rew_mean     | -2.5     |\n",
            "| time/              |          |\n",
            "|    fps             | 6547     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 0        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.5         |\n",
            "|    ep_rew_mean          | -2.36       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 4837        |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 0           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016748298 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.19       |\n",
            "|    explained_variance   | -0.198      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.463       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0339     |\n",
            "|    value_loss           | 2.19        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.66        |\n",
            "|    ep_rew_mean          | -2.03       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 4429        |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 1           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015495773 |\n",
            "|    clip_fraction        | 0.135       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.16       |\n",
            "|    explained_variance   | -0.0224     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.506       |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0315     |\n",
            "|    value_loss           | 1.22        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 3.84         |\n",
            "|    ep_rew_mean          | -1.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 4165         |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 1            |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0143419495 |\n",
            "|    clip_fraction        | 0.12         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.12        |\n",
            "|    explained_variance   | -0.0161      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.808        |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.0323      |\n",
            "|    value_loss           | 1.72         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.97        |\n",
            "|    ep_rew_mean          | -1.51       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 4088        |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 2           |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015683487 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.06       |\n",
            "|    explained_variance   | -0.0131     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.859       |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0392     |\n",
            "|    value_loss           | 1.98        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.23        |\n",
            "|    ep_rew_mean          | -1.21       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3848        |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 3           |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013737302 |\n",
            "|    clip_fraction        | 0.14        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.99       |\n",
            "|    explained_variance   | -0.00712    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.879       |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0368     |\n",
            "|    value_loss           | 1.99        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.07        |\n",
            "|    ep_rew_mean          | -1.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3782        |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 3           |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013240514 |\n",
            "|    clip_fraction        | 0.117       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.93       |\n",
            "|    explained_variance   | -0.00695    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.18        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0341     |\n",
            "|    value_loss           | 2.21        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 4.07         |\n",
            "|    ep_rew_mean          | -0.47        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 3744         |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0128440745 |\n",
            "|    clip_fraction        | 0.138        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.86        |\n",
            "|    explained_variance   | -0.0113      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.855        |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.0324      |\n",
            "|    value_loss           | 2.02         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4           |\n",
            "|    ep_rew_mean          | -0.69       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3730        |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 4           |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012656143 |\n",
            "|    clip_fraction        | 0.135       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.81       |\n",
            "|    explained_variance   | -0.00363    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.984       |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0323     |\n",
            "|    value_loss           | 1.96        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.22        |\n",
            "|    ep_rew_mean          | -0.39       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3718        |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012371648 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.76       |\n",
            "|    explained_variance   | 0.0096      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.867       |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0346     |\n",
            "|    value_loss           | 1.83        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.26        |\n",
            "|    ep_rew_mean          | -0.27       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3733        |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013570161 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.7        |\n",
            "|    explained_variance   | 0.00412     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.717       |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0353     |\n",
            "|    value_loss           | 1.78        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 4.14       |\n",
            "|    ep_rew_mean          | -0.18      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 3741       |\n",
            "|    iterations           | 12         |\n",
            "|    time_elapsed         | 6          |\n",
            "|    total_timesteps      | 24576      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01273297 |\n",
            "|    clip_fraction        | 0.145      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.65      |\n",
            "|    explained_variance   | 0.0147     |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.795      |\n",
            "|    n_updates            | 110        |\n",
            "|    policy_gradient_loss | -0.0299    |\n",
            "|    value_loss           | 1.76       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.02        |\n",
            "|    ep_rew_mean          | -0.27       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3749        |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013722268 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.6        |\n",
            "|    explained_variance   | 0.035       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.753       |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0297     |\n",
            "|    value_loss           | 1.57        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.2         |\n",
            "|    ep_rew_mean          | -0.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3738        |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010280672 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.57       |\n",
            "|    explained_variance   | 0.039       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.815       |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0268     |\n",
            "|    value_loss           | 1.44        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.14        |\n",
            "|    ep_rew_mean          | 0.11        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3747        |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010997404 |\n",
            "|    clip_fraction        | 0.125       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.51       |\n",
            "|    explained_variance   | 0.0287      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.721       |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0259     |\n",
            "|    value_loss           | 1.32        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.14        |\n",
            "|    ep_rew_mean          | 0.17        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3755        |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 8           |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011860207 |\n",
            "|    clip_fraction        | 0.144       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.48       |\n",
            "|    explained_variance   | 0.0404      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.449       |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0278     |\n",
            "|    value_loss           | 1.18        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.18        |\n",
            "|    ep_rew_mean          | 0.18        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3740        |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012103477 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.0535      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.421       |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0256     |\n",
            "|    value_loss           | 1.09        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.16        |\n",
            "|    ep_rew_mean          | 0.27        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3746        |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011596192 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0.049       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.534       |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0237     |\n",
            "|    value_loss           | 1           |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.15        |\n",
            "|    ep_rew_mean          | 0.42        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3749        |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 10          |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012225196 |\n",
            "|    clip_fraction        | 0.122       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.31       |\n",
            "|    explained_variance   | 0.0375      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.58        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.0245     |\n",
            "|    value_loss           | 1.03        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 3.97       |\n",
            "|    ep_rew_mean          | 0.27       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 3751       |\n",
            "|    iterations           | 20         |\n",
            "|    time_elapsed         | 10         |\n",
            "|    total_timesteps      | 40960      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01385181 |\n",
            "|    clip_fraction        | 0.143      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.24      |\n",
            "|    explained_variance   | 0.0693     |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.486      |\n",
            "|    n_updates            | 190        |\n",
            "|    policy_gradient_loss | -0.0257    |\n",
            "|    value_loss           | 0.904      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.08        |\n",
            "|    ep_rew_mean          | 0.49        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3746        |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010409825 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.2        |\n",
            "|    explained_variance   | 0.0312      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.363       |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0234     |\n",
            "|    value_loss           | 0.966       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4           |\n",
            "|    ep_rew_mean          | 0.26        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3752        |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011801737 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.0681      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.397       |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.0278     |\n",
            "|    value_loss           | 1.02        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 4.11         |\n",
            "|    ep_rew_mean          | 0.33         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 3758         |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 47104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0096286945 |\n",
            "|    clip_fraction        | 0.117        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.14        |\n",
            "|    explained_variance   | 0.0592       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.557        |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.0238      |\n",
            "|    value_loss           | 0.944        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.03        |\n",
            "|    ep_rew_mean          | 0.16        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3762        |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009912677 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.0616      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.185       |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0196     |\n",
            "|    value_loss           | 0.838       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.1         |\n",
            "|    ep_rew_mean          | 0.35        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3755        |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010726062 |\n",
            "|    clip_fraction        | 0.119       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.05       |\n",
            "|    explained_variance   | 0.057       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.308       |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0229     |\n",
            "|    value_loss           | 0.847       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.12        |\n",
            "|    ep_rew_mean          | 0.44        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3758        |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011514094 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0.0607      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.404       |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.0209     |\n",
            "|    value_loss           | 0.835       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.02        |\n",
            "|    ep_rew_mean          | 0.38        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3764        |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008384034 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.95       |\n",
            "|    explained_variance   | 0.105       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.267       |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.0208     |\n",
            "|    value_loss           | 0.746       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.97        |\n",
            "|    ep_rew_mean          | 0.64        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3754        |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 15          |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010553708 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.887      |\n",
            "|    explained_variance   | 0.112       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.213       |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0214     |\n",
            "|    value_loss           | 0.607       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.07        |\n",
            "|    ep_rew_mean          | 0.51        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3759        |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 15          |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008899187 |\n",
            "|    clip_fraction        | 0.0923      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.887      |\n",
            "|    explained_variance   | 0.0815      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.226       |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.0178     |\n",
            "|    value_loss           | 0.765       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.97        |\n",
            "|    ep_rew_mean          | 0.57        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3750        |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 16          |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008741604 |\n",
            "|    clip_fraction        | 0.088       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.857      |\n",
            "|    explained_variance   | 0.0611      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.231       |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.0189     |\n",
            "|    value_loss           | 0.849       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.89        |\n",
            "|    ep_rew_mean          | 0.44        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3737        |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 16          |\n",
            "|    total_timesteps      | 63488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008369034 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.84       |\n",
            "|    explained_variance   | 0.0829      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.204       |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.0186     |\n",
            "|    value_loss           | 0.592       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.95        |\n",
            "|    ep_rew_mean          | 0.75        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3726        |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009622198 |\n",
            "|    clip_fraction        | 0.094       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.862      |\n",
            "|    explained_variance   | 0.0663      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.493       |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.0194     |\n",
            "|    value_loss           | 0.856       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.94        |\n",
            "|    ep_rew_mean          | 0.65        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3729        |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009701653 |\n",
            "|    clip_fraction        | 0.0849      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.837      |\n",
            "|    explained_variance   | 0.0485      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.151       |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.018      |\n",
            "|    value_loss           | 0.618       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.97        |\n",
            "|    ep_rew_mean          | 0.61        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3731        |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 18          |\n",
            "|    total_timesteps      | 69632       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010468101 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.82       |\n",
            "|    explained_variance   | 0.0564      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.284       |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | -0.0193     |\n",
            "|    value_loss           | 0.623       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.99        |\n",
            "|    ep_rew_mean          | 0.61        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3721        |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 19          |\n",
            "|    total_timesteps      | 71680       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009932658 |\n",
            "|    clip_fraction        | 0.109       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.788      |\n",
            "|    explained_variance   | 0.0726      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.331       |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | -0.019      |\n",
            "|    value_loss           | 0.577       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.85        |\n",
            "|    ep_rew_mean          | 0.58        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3719        |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 19          |\n",
            "|    total_timesteps      | 73728       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008926311 |\n",
            "|    clip_fraction        | 0.0947      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.78       |\n",
            "|    explained_variance   | 0.122       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.373       |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | -0.0154     |\n",
            "|    value_loss           | 0.493       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.11        |\n",
            "|    ep_rew_mean          | 0.62        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3720        |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 20          |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008112893 |\n",
            "|    clip_fraction        | 0.0873      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.779      |\n",
            "|    explained_variance   | 0.108       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.191       |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.0161     |\n",
            "|    value_loss           | 0.5         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.88        |\n",
            "|    ep_rew_mean          | 0.67        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3718        |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 20          |\n",
            "|    total_timesteps      | 77824       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012053552 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.765      |\n",
            "|    explained_variance   | 0.102       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.371       |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.0203     |\n",
            "|    value_loss           | 0.57        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 4.03        |\n",
            "|    ep_rew_mean          | 0.54        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3714        |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 21          |\n",
            "|    total_timesteps      | 79872       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009662323 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.755      |\n",
            "|    explained_variance   | 0.119       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.272       |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.0187     |\n",
            "|    value_loss           | 0.548       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 3.92         |\n",
            "|    ep_rew_mean          | 0.58         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 3721         |\n",
            "|    iterations           | 40           |\n",
            "|    time_elapsed         | 22           |\n",
            "|    total_timesteps      | 81920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071377773 |\n",
            "|    clip_fraction        | 0.0873       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.764       |\n",
            "|    explained_variance   | 0.142        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.144        |\n",
            "|    n_updates            | 390          |\n",
            "|    policy_gradient_loss | -0.0165      |\n",
            "|    value_loss           | 0.552        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.98        |\n",
            "|    ep_rew_mean          | 0.77        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3718        |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 22          |\n",
            "|    total_timesteps      | 83968       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011508966 |\n",
            "|    clip_fraction        | 0.123       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.74       |\n",
            "|    explained_variance   | 0.101       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.324       |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | -0.0234     |\n",
            "|    value_loss           | 0.617       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.96        |\n",
            "|    ep_rew_mean          | 0.53        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3723        |\n",
            "|    iterations           | 42          |\n",
            "|    time_elapsed         | 23          |\n",
            "|    total_timesteps      | 86016       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009436054 |\n",
            "|    clip_fraction        | 0.106       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.727      |\n",
            "|    explained_variance   | 0.09        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.233       |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.0167     |\n",
            "|    value_loss           | 0.406       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.99        |\n",
            "|    ep_rew_mean          | 0.54        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3717        |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 23          |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011136174 |\n",
            "|    clip_fraction        | 0.125       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.718      |\n",
            "|    explained_variance   | 0.129       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.188       |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.0187     |\n",
            "|    value_loss           | 0.505       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 3.94       |\n",
            "|    ep_rew_mean          | 0.55       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 3722       |\n",
            "|    iterations           | 44         |\n",
            "|    time_elapsed         | 24         |\n",
            "|    total_timesteps      | 90112      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01159339 |\n",
            "|    clip_fraction        | 0.135      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.717     |\n",
            "|    explained_variance   | 0.128      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.14       |\n",
            "|    n_updates            | 430        |\n",
            "|    policy_gradient_loss | -0.0197    |\n",
            "|    value_loss           | 0.476      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.91        |\n",
            "|    ep_rew_mean          | 0.62        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3726        |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 24          |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008809611 |\n",
            "|    clip_fraction        | 0.0783      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.713      |\n",
            "|    explained_variance   | 0.102       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.338       |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.0162     |\n",
            "|    value_loss           | 0.546       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 3.93      |\n",
            "|    ep_rew_mean          | 0.69      |\n",
            "| time/                   |           |\n",
            "|    fps                  | 3723      |\n",
            "|    iterations           | 46        |\n",
            "|    time_elapsed         | 25        |\n",
            "|    total_timesteps      | 94208     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0106056 |\n",
            "|    clip_fraction        | 0.119     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.65     |\n",
            "|    explained_variance   | 0.112     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.224     |\n",
            "|    n_updates            | 450       |\n",
            "|    policy_gradient_loss | -0.0213   |\n",
            "|    value_loss           | 0.396     |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.91        |\n",
            "|    ep_rew_mean          | 0.67        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3726        |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 25          |\n",
            "|    total_timesteps      | 96256       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008791332 |\n",
            "|    clip_fraction        | 0.0988      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.635      |\n",
            "|    explained_variance   | 0.152       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.103       |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.0163     |\n",
            "|    value_loss           | 0.462       |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 3.95      |\n",
            "|    ep_rew_mean          | 0.77      |\n",
            "| time/                   |           |\n",
            "|    fps                  | 3726      |\n",
            "|    iterations           | 48        |\n",
            "|    time_elapsed         | 26        |\n",
            "|    total_timesteps      | 98304     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0066606 |\n",
            "|    clip_fraction        | 0.0898    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.645    |\n",
            "|    explained_variance   | 0.114     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.275     |\n",
            "|    n_updates            | 470       |\n",
            "|    policy_gradient_loss | -0.0163   |\n",
            "|    value_loss           | 0.418     |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.89        |\n",
            "|    ep_rew_mean          | 0.83        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 3727        |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 26          |\n",
            "|    total_timesteps      | 100352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008640661 |\n",
            "|    clip_fraction        | 0.0911      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.645      |\n",
            "|    explained_variance   | 0.103       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.292       |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | -0.0155     |\n",
            "|    value_loss           | 0.417       |\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = PPO(\"MlpPolicy\", env, verbose=1).learn(100000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
